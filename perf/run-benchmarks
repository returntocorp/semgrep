#! /usr/bin/env python3
#
# Run semgrep on a series of pairs (rules, repo) with different options,
# and report the time it takes. Optionally upload the results to the semgrep
# dashboard.
#
# With the --semgrep_core option, instead run semgrep-core on a series of
# pairs (rules, repo) with options chosen to test semgrep-core performance.
# Note that semgrep-core can currently be run for one language only, so
# these benchmarks only include corpuses that are primarily one language.
# This allows them to be compared to the semgrep runtimes. Can also upload
# the results to the dashboard, and use a dummy set instead
#
import argparse
import json
import os
import subprocess
import time
import urllib.request
from contextlib import contextmanager
from typing import Iterator
from typing import Tuple

import matplotlib.pyplot as plt
import pandas as pd
import semgrep_core_benchmark  # type: ignore

DASHBOARD_URL = "https://dashboard.semgrep.dev"

# Run command and propagate errors
def cmd(*args: str) -> None:
    subprocess.run(args, check=True)  # nosem


class Corpus:
    def __init__(self, name: str, rule_dir: str, target_dir: str):
        # name for the input corpus (rules and targets)
        self.name = name

        # folder containing the semgrep rules
        self.rule_dir = rule_dir

        # folder containing the target source files
        self.target_dir = target_dir

    # Fetch rules and targets is delegated to an ad-hoc script named 'prep'.
    def prep(self) -> None:
        cmd("./prep")


CORPUSES = [
    # Run Ajin's nodejsscan rules on some repo containing javascript files.
    # This takes something like 4 hours or more. Maybe we could run it
    # on fewer targets.
    # Corpus("njs", "input/njsscan/njsscan/rules/semantic_grep", "input/juice-shop"),
    Corpus("big-js", "input/semgrep.yml", "input/big-js"),
    Corpus(
        "njsbox", "input/njsscan/njsscan/rules/semantic_grep", "input/dropbox-sdk-js"
    ),
    Corpus("zulip", "input/semgrep.yml", "input/zulip"),
    # The tests below all run r2c rulepacks (in r2c-rules) on public repos
    # For command Corpus("$X", ..., "input/$Y"), you can find the repo by
    # going to github.com/$X/$Y
    #
    # Run our django rulepack on a large python repo
    Corpus("apache", "input/django.yml", "input/libcloud"),
    # Run our flask rulepack on a python repo
    Corpus("dropbox", "input/flask.yml", "input/pytest-flakefinder"),
    # Run our r2c-ci and r2c-security-audit packs on a go/ruby repo
    Corpus("coinbase", "input/rules", "input/bifrost"),
    # Run our r2c-ci and r2c-security audit packs on a python/JS repo
    Corpus("netflix", "input/rules", "input/lemur"),
    # Run our r2c-ci and r2c-security audit packs on a JS/other repo
    Corpus("draios", "input/rules", "input/sysdig-inspect"),
    # Run our golang rulepack on a go/html repo
    Corpus("0c34", "input/golang.yml", "input/govwa"),
    # Run our ruby rulepack on a large ruby repo
    Corpus("rails", "input/ruby.yml", "input/rails"),
    # Run our javascript and eslint-plugin-security packs on a large JS repo
    Corpus("lodash", "input/rules", "input/lodash"),
]

DUMMY_CORPUSES = [Corpus("dummy", "input/dummy/rules", "input/dummy/targets")]

# Projects GitLab is using to benchmark us on perf
GITLAB_CORPUSES = [
    # (Gitlab large) Run our javascript and r2c-secuirty audit packs on a js/ruby repo
    # Corpus("gitlab", "input/rules", "input/gitlab"),
    # (Gitlab large) Run our security-audit pack on a c repo
    # Corpus("smacker", "input/r2c-security-audit.yml", "input/gotree"),
    # (Gitlab large) Run our java pack on a java repo
    # Corpus("spring-projects", "input/java.yml", "input/spring"),
    # (Gitlab medium) Run our python and flask packs on a python repo
    Corpus("django", "input/rules", "input/django"),
    # (Gitlab medium) Run our r2c-ci and r2c-security audit packs on a java repo
    Corpus("pmd", "input/rules", "input/pmd"),
    # (Gitlab medium) Run our r2c-ci and r2c-security audit packs on a java repo
    Corpus("dropwizard", "input/rules", "input/dropwizard"),
    # (Gitlab small) Run our python and flask rules on a python repo
    Corpus("pallets", "input/rules", "input/flask"),
    # (Gitlab small) Run our javascript rules on a JS repo
    Corpus("socketio", "input/javascript.yml", "input/socket"),
]

# For corpuses that cannot be run in CI because they use private repos
INTERNAL_CORPUSES = [
    # Run our javascript and eslint-plugin-security packs on a large JS repo
    Corpus("lodash", "input/rules", "input/lodash"),
    # Corpus("dogfood", "input/semgrep.yml", "input/"),
]


class SemgrepVariant:
    def __init__(self, name: str, semgrep_core_extra: str, semgrep_extra: str = ""):
        # name for the input corpus (rules and targets)
        self.name = name

        # space-separated extra arguments to pass to semgrep-core
        # command via SEMGREP_CORE_EXTRA environment variable
        self.semgrep_core_extra = semgrep_core_extra

        # space-separated extra arguments to pass to the default semgrep
        # command
        self.semgrep_extra = semgrep_extra


# Feel free to create new variants. The idea is to use the default set
# of options as the baseline and we see what happens when we enable or
# disable this or that optimization.
#
SEMGREP_VARIANTS = [
    # default settings
    SemgrepVariant("std", ""),
    # SemgrepVariant("no-cache", "-no_opt_cache"),
    # SemgrepVariant("max-cache", "-opt_max_cache"),
    # SemgrepVariant("no-bloom", "-no_bloom_filter"),
    # SemgrepVariant("no-gc-tuning", "-no_gc_tuning"),
    # SemgrepVariant("set_filters", "-set_filter"),
    SemgrepVariant("experimental", "-fast", "--experimental"),
]


class SemgrepResult:
    def __init__(self, dict: dict) -> None:
        self.res = dict

    def __eq__(self, other: object) -> bool:
        if not isinstance(other, SemgrepResult):
            raise NotImplementedError

        def compare_locs(s_loc: dict, o_loc: dict) -> bool:
            if s_loc is not None and o_loc is not None:
                return bool(
                    s_loc["line"] == o_loc["line"] and s_loc["col"] == o_loc["col"]
                )
            else:
                return False

        def compare_extras(other: SemgrepResult) -> bool:
            if "extra" in self.res and "extra" in other.res:
                return True
                # return json.dumps(self.res["extra"]) == json.dumps(other["extra"])
            else:
                return "extra" not in self.res and "extra" not in other.res

        output = (
            self.res["check_id"] == other.res["check_id"]
            and self.res["path"] == other.res["path"]
            and compare_locs(self.res["start"], other.res["start"])
            and compare_locs(self.res["end"], other.res["end"])
            and compare_extras(other)
        )
        print("Result = ", output)
        return output

    def __hash__(self) -> int:
        return hash(json.dumps(self.res))


# Add support for: with chdir(DIR): ...
@contextmanager
def chdir(dir: str) -> Iterator[None]:
    old_dir = os.getcwd()
    os.chdir(dir)
    try:
        yield
    finally:
        os.chdir(old_dir)


def upload_results(metric_name: str, value: float) -> None:
    url = f"{DASHBOARD_URL}/api/metric/{metric_name}"
    print(f"Uploading to {url}")
    r = urllib.request.urlopen(  # nosem
        url=url,
        data=str(value).encode("ascii"),
    )
    print(r.read().decode())


def standardize_findings(findings: dict) -> dict:
    if "errors" not in findings:
        msg = json.dumps(findings, indent=4) + "\n\nDid not find expected key 'errors'"
        raise Exception(msg)
    if "results" not in findings:
        msg = json.dumps(findings, indent=4) + "\n\nDid not find expected key 'results'"
        raise Exception(msg)
    return {
        "errors": findings["errors"],
        "results": {SemgrepResult(i) for i in findings["results"]},
    }


def output_differences(
    findings: set, std_findings: set, variant: str
) -> Tuple[int, int]:
    def output_diff(diff: set) -> None:
        for d in diff:
            print(json.dumps(d, indent=4))

    f_diff = findings.difference(std_findings)
    s_diff = std_findings.difference(findings)
    fd_len = len(f_diff)
    sd_len = len(s_diff)
    print("In", variant, "but not std", fd_len, "findings :")
    output_diff(f_diff)
    print("In std but not", variant, sd_len, "findings :")
    output_diff(s_diff)
    return fd_len, sd_len


def run_semgrep(
    docker: str, corpus: Corpus, variant: SemgrepVariant
) -> Tuple[float, bytes]:
    args = []
    common_args = [
        "--strict",
        "--json",
        "--timeout",
        "0",
        "--verbose",
        "--no-git-ignore",  # because files in bench/*/input/ are git-ignored
    ]
    if docker:
        # Absolute paths are required by docker for mounting volumes, otherwise
        # they end up empty inside the container.
        args = [
            "docker",
            "run",
            "-v",
            os.path.abspath(corpus.rule_dir) + ":/rules",
            "-v",
            os.path.abspath(corpus.target_dir) + ":/targets",
            "-t",
            docker,
            "--config",
            "/rules",
            "/targets",
        ]
    else:
        # Absolute paths for rules and targets are required by semgrep
        # when running within the semgrep docker container.
        args = [
            "semgrep",
            "--config",
            os.path.abspath(corpus.rule_dir),
            os.path.abspath(corpus.target_dir),
        ]
    args.extend(common_args)
    if variant.semgrep_extra != "":
        args.extend([variant.semgrep_extra])

    print(f"current directory: {os.getcwd()}")
    print("semgrep command: {}".format(" ".join(args)))
    os.environ["SEMGREP_CORE_EXTRA"] = variant.semgrep_core_extra
    print(f"extra arguments for semgrep-core: '{variant.semgrep_core_extra}'")

    t1 = time.time()
    res = subprocess.run(args, capture_output=True)  # nosem
    t2 = time.time()

    status = res.returncode
    print(f"semgrep exit status: {status}")
    if status == 0:
        print("success")
    elif status == 3:
        print("warning: some files couldn't be parsed")
    else:
        res.check_returncode()

    return t2 - t1, res.stdout


def run_benchmarks(
    docker: str,
    dummy: bool,
    gitlab: bool,
    internal: bool,
    plot_benchmarks: bool,
    upload: bool,
) -> None:
    results_msgs = []
    results: dict = {variant.name: [] for variant in SEMGREP_VARIANTS}

    corpuses = CORPUSES
    if dummy:
        corpuses = DUMMY_CORPUSES
    if internal:
        corpuses = INTERNAL_CORPUSES
    if gitlab:
        corpuses = GITLAB_CORPUSES
    for corpus in corpuses:
        with chdir(corpus.name):
            corpus.prep()
            std_findings = {}
            for variant in SEMGREP_VARIANTS:
                # Run variant
                name = ".".join(["semgrep", "bench", corpus.name, variant.name])
                metric_name = ".".join([name, "duration"])
                print(f"------ {name} ------")
                duration, findings_bytes = run_semgrep(docker, corpus, variant)

                # Report results
                msg = f"{metric_name} = {duration:.3f} s"
                print(msg)
                results_msgs.append(msg)
                results[variant.name].append(duration)

                if upload:
                    upload_results(metric_name, duration)

                # Check correctness
                findings = standardize_findings(json.loads(findings_bytes))

                num_results = len(findings["results"])
                num_errors = len(findings["errors"])
                print(f"Result: {num_results} findings, {num_errors} parse errors")

                print("Checking symmetric difference")
                if variant.name == "std":
                    std_findings = findings
                elif findings["results"] ^ std_findings["results"] != set():
                    print("found difference")
                    fd_len, sd_len = output_differences(
                        findings["results"], std_findings["results"], variant.name
                    )
                    results_msgs[
                        -1
                    ] += f" ERROR: {fd_len} extra findings, {sd_len} missing findings"
                elif len(findings["errors"]) > len(std_findings["errors"]):
                    results_msgs[-1] += " WARNING: more errors than std"

    # Show summary data
    for msg in results_msgs:
        print(msg)
    if plot_benchmarks:
        indexes = [corpus.name for corpus in corpuses]
        plotdata = pd.DataFrame(results, index=indexes)
        plotdata.plot(kind="bar")
        plt.show()


def main() -> None:
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--docker",
        metavar="DOCKER_IMAGE",
        type=str,
        help="use the specified docker image for semgrep, such as returntocorp/semgrep:develop",
    )
    parser.add_argument(
        "--dummy",
        help="run quick, fake benchmarks for development purposes",
        action="store_true",
    )
    parser.add_argument(
        "--internal",
        help="run internal benchmarks for development purposes",
        action="store_true",
    )
    parser.add_argument(
        "--gitlab",
        help="run gitlab benchmarks for development purposes",
        action="store_true",
    )
    parser.add_argument(
        "--upload", help="upload results to semgrep dashboard", action="store_true"
    )
    parser.add_argument(
        "--plot_benchmarks",
        help="display a graph of the benchmark results",
        action="store_true",
    )
    parser.add_argument(
        "--semgrep_core", help="run semgrep-core benchmarks", action="store_true"
    )
    args = parser.parse_args()
    with chdir("bench"):
        if args.semgrep_core:
            semgrep_core_benchmark.run_benchmarks(args.dummy, args.upload)
        else:
            run_benchmarks(
                args.docker,
                args.dummy,
                args.gitlab,
                args.internal,
                args.plot_benchmarks,
                args.upload,
            )


if __name__ == "__main__":
    main()
